{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kMI_LirhZGkX",
        "outputId": "c00c38c8-27ff-4790-d54d-28a839772867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting FMAD-based training...\n",
            "\n",
            "=== FMAD Epoch 1/10 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1230269/3377374294.py:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Batch 500/4487: Loss = 6.7012\n",
            "  Batch 1000/4487: Loss = 6.7360\n",
            "  Batch 1500/4487: Loss = 6.5546\n",
            "  Batch 2000/4487: Loss = 6.8565\n",
            "  Batch 2500/4487: Loss = 6.4651\n",
            "  Batch 3000/4487: Loss = 6.2516\n",
            "  Batch 3500/4487: Loss = 6.4660\n",
            "  Batch 4000/4487: Loss = 6.2381\n",
            "  Batch 4487/4487: Loss = 6.3543\n",
            "FMAD epoch completed in 917.20 seconds.\n",
            "FMAD Training Loss for epoch: 6.4670678153185746\n",
            "Epoch 1 finished in 15 minutes 17.20 seconds.\n",
            "FMAD Training Loss: 6.4671\n",
            "\n",
            "\n",
            "=== FMAD Epoch 2/10 ===\n",
            "  Batch 500/4487: Loss = 6.1095\n",
            "  Batch 1000/4487: Loss = 6.2811\n",
            "  Batch 1500/4487: Loss = 6.1863\n",
            "  Batch 2000/4487: Loss = 6.0714\n",
            "  Batch 2500/4487: Loss = 5.9475\n",
            "  Batch 3000/4487: Loss = 5.9162\n",
            "  Batch 3500/4487: Loss = 6.1775\n",
            "  Batch 4000/4487: Loss = 5.9865\n",
            "  Batch 4487/4487: Loss = 6.6216\n",
            "FMAD epoch completed in 917.75 seconds.\n",
            "FMAD Training Loss for epoch: 6.148205031949055\n",
            "Epoch 2 finished in 15 minutes 17.75 seconds.\n",
            "FMAD Training Loss: 6.1482\n",
            "\n",
            "\n",
            "=== FMAD Epoch 3/10 ===\n",
            "  Batch 500/4487: Loss = 5.9321\n",
            "  Batch 1000/4487: Loss = 6.0801\n",
            "  Batch 1500/4487: Loss = 6.0345\n",
            "  Batch 2000/4487: Loss = 6.1317\n",
            "  Batch 2500/4487: Loss = 6.2533\n",
            "  Batch 3000/4487: Loss = 6.0118\n",
            "  Batch 3500/4487: Loss = 6.5771\n",
            "  Batch 4000/4487: Loss = 6.0907\n",
            "  Batch 4487/4487: Loss = 5.7712\n",
            "FMAD epoch completed in 1121.73 seconds.\n",
            "FMAD Training Loss for epoch: 6.0709148286471315\n",
            "Epoch 3 finished in 18 minutes 41.73 seconds.\n",
            "FMAD Training Loss: 6.0709\n",
            "\n",
            "\n",
            "=== FMAD Epoch 4/10 ===\n",
            "  Batch 500/4487: Loss = 5.9106\n",
            "  Batch 1000/4487: Loss = 5.8219\n",
            "  Batch 1500/4487: Loss = 6.2535\n",
            "  Batch 2000/4487: Loss = 5.8012\n",
            "  Batch 2500/4487: Loss = 6.1189\n",
            "  Batch 3000/4487: Loss = 6.1694\n",
            "  Batch 3500/4487: Loss = 5.9820\n",
            "  Batch 4000/4487: Loss = 5.9024\n",
            "  Batch 4487/4487: Loss = 6.0186\n",
            "FMAD epoch completed in 1028.97 seconds.\n",
            "FMAD Training Loss for epoch: 6.013663668980013\n",
            "Epoch 4 finished in 17 minutes 8.97 seconds.\n",
            "FMAD Training Loss: 6.0137\n",
            "\n",
            "\n",
            "=== FMAD Epoch 5/10 ===\n",
            "  Batch 500/4487: Loss = 5.8982\n",
            "  Batch 1000/4487: Loss = 5.4304\n",
            "  Batch 1500/4487: Loss = 6.0426\n",
            "  Batch 2000/4487: Loss = 5.9849\n",
            "  Batch 2500/4487: Loss = 5.8401\n",
            "  Batch 3000/4487: Loss = 5.9111\n",
            "  Batch 3500/4487: Loss = 6.0030\n",
            "  Batch 4000/4487: Loss = 5.8983\n",
            "  Batch 4487/4487: Loss = 6.3526\n",
            "FMAD epoch completed in 924.19 seconds.\n",
            "FMAD Training Loss for epoch: 5.975928711258863\n",
            "Epoch 5 finished in 15 minutes 24.19 seconds.\n",
            "FMAD Training Loss: 5.9759\n",
            "\n",
            "\n",
            "=== FMAD Epoch 6/10 ===\n",
            "  Batch 500/4487: Loss = 5.5360\n",
            "  Batch 1000/4487: Loss = 5.9119\n",
            "  Batch 1500/4487: Loss = 6.1011\n",
            "  Batch 2000/4487: Loss = 6.0376\n",
            "  Batch 2500/4487: Loss = 5.8442\n",
            "  Batch 3000/4487: Loss = 5.9036\n",
            "  Batch 3500/4487: Loss = 6.3517\n",
            "  Batch 4000/4487: Loss = 5.8391\n",
            "  Batch 4487/4487: Loss = 5.9814\n",
            "FMAD epoch completed in 921.03 seconds.\n",
            "FMAD Training Loss for epoch: 5.94188760526201\n",
            "Epoch 6 finished in 15 minutes 21.03 seconds.\n",
            "FMAD Training Loss: 5.9419\n",
            "\n",
            "\n",
            "=== FMAD Epoch 7/10 ===\n",
            "  Batch 500/4487: Loss = 5.6526\n",
            "  Batch 1000/4487: Loss = 5.8914\n",
            "  Batch 1500/4487: Loss = 6.1394\n",
            "  Batch 2000/4487: Loss = 5.9910\n",
            "  Batch 2500/4487: Loss = 5.8487\n",
            "  Batch 3000/4487: Loss = 5.9523\n",
            "  Batch 3500/4487: Loss = 5.8820\n",
            "  Batch 4000/4487: Loss = 6.0993\n",
            "  Batch 4487/4487: Loss = 5.7968\n",
            "FMAD epoch completed in 921.93 seconds.\n",
            "FMAD Training Loss for epoch: 5.916386416634818\n",
            "Epoch 7 finished in 15 minutes 21.93 seconds.\n",
            "FMAD Training Loss: 5.9164\n",
            "\n",
            "\n",
            "=== FMAD Epoch 8/10 ===\n",
            "  Batch 500/4487: Loss = 5.7006\n",
            "  Batch 1000/4487: Loss = 6.0938\n",
            "  Batch 1500/4487: Loss = 5.7099\n",
            "  Batch 2000/4487: Loss = 5.6782\n",
            "  Batch 2500/4487: Loss = 5.7218\n",
            "  Batch 3000/4487: Loss = 6.0795\n",
            "  Batch 3500/4487: Loss = 5.6902\n",
            "  Batch 4000/4487: Loss = 6.3137\n",
            "  Batch 4487/4487: Loss = 5.3284\n",
            "FMAD epoch completed in 920.14 seconds.\n",
            "FMAD Training Loss for epoch: 5.894259388738521\n",
            "Epoch 8 finished in 15 minutes 20.14 seconds.\n",
            "FMAD Training Loss: 5.8943\n",
            "\n",
            "\n",
            "=== FMAD Epoch 9/10 ===\n",
            "  Batch 500/4487: Loss = 5.8208\n",
            "  Batch 1000/4487: Loss = 5.9224\n",
            "  Batch 1500/4487: Loss = 5.8496\n",
            "  Batch 2000/4487: Loss = 5.7706\n",
            "  Batch 2500/4487: Loss = 5.8701\n",
            "  Batch 3000/4487: Loss = 6.0400\n",
            "  Batch 3500/4487: Loss = 5.9219\n",
            "  Batch 4000/4487: Loss = 5.8985\n",
            "  Batch 4487/4487: Loss = 6.7272\n",
            "FMAD epoch completed in 920.57 seconds.\n",
            "FMAD Training Loss for epoch: 5.881491266381961\n",
            "Epoch 9 finished in 15 minutes 20.57 seconds.\n",
            "FMAD Training Loss: 5.8815\n",
            "\n",
            "\n",
            "=== FMAD Epoch 10/10 ===\n",
            "  Batch 500/4487: Loss = 5.7165\n",
            "  Batch 1000/4487: Loss = 5.9881\n",
            "  Batch 1500/4487: Loss = 6.0484\n",
            "  Batch 2000/4487: Loss = 5.8041\n",
            "  Batch 2500/4487: Loss = 5.8179\n",
            "  Batch 3000/4487: Loss = 5.8588\n",
            "  Batch 3500/4487: Loss = 6.1074\n",
            "  Batch 4000/4487: Loss = 5.8624\n",
            "  Batch 4487/4487: Loss = 5.2256\n",
            "FMAD epoch completed in 922.05 seconds.\n",
            "FMAD Training Loss for epoch: 5.873652181780521\n",
            "Epoch 10 finished in 15 minutes 22.05 seconds.\n",
            "FMAD Training Loss: 5.8737\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizer\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "\n",
        "# Enable cuDNN benchmarking for performance.\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# --------------------- Data Preparation ---------------------\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, dataset_split, tokenizer, max_input_length=256, max_target_length=128):\n",
        "        self.dataset = dataset_split\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        source = item['article']\n",
        "        target = item['highlights']\n",
        "        source_enc = self.tokenizer(\n",
        "            source, truncation=True, padding='max_length',\n",
        "            max_length=self.max_input_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        target_enc = self.tokenizer(\n",
        "            target, truncation=True, padding='max_length',\n",
        "            max_length=self.max_target_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': source_enc.input_ids.squeeze(0),\n",
        "            'attention_mask': source_enc.attention_mask.squeeze(0),\n",
        "            'target_ids': target_enc.input_ids.squeeze(0)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    target_ids = torch.stack([item['target_ids'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'target_ids': target_ids}\n",
        "\n",
        "# --------------------- Model Definition ---------------------\n",
        "class Seq2SeqGRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_idx, dropout=0.3):\n",
        "        super(Seq2SeqGRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.encoder = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # Encode source sequence\n",
        "        embedded_src = self.embedding(src)\n",
        "        embedded_src = self.dropout(embedded_src)\n",
        "        encoder_outputs, hidden = self.encoder(embedded_src)\n",
        "        # Decode target sequence using teacher forcing\n",
        "        embedded_trg = self.embedding(trg)\n",
        "        embedded_trg = self.dropout(embedded_trg)\n",
        "        decoder_outputs, _ = self.decoder(embedded_trg, hidden)\n",
        "        decoder_outputs = self.dropout(decoder_outputs)\n",
        "        output = self.fc(decoder_outputs)\n",
        "        return output\n",
        "\n",
        "    def generate(self, src, sos_token, eos_token, max_len=128, beam_width=3):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            embedded_src = self.embedding(src)\n",
        "            embedded_src = self.dropout(embedded_src)\n",
        "            encoder_outputs, hidden = self.encoder(embedded_src)\n",
        "            batch_size = src.size(0)\n",
        "\n",
        "            # Initialize beams for each sample in the batch.\n",
        "            beams = [\n",
        "                [(torch.tensor([sos_token], device=src.device), 0.0, hidden[:, i:i+1, :])]\n",
        "                for i in range(batch_size)\n",
        "            ]\n",
        "\n",
        "            final_outputs = [None] * batch_size\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                new_beams = []\n",
        "                all_finished = True\n",
        "\n",
        "                for i in range(batch_size):\n",
        "                    temp_beams = []\n",
        "                    for seq, score, h in beams[i]:\n",
        "                        # If the last token is EOS, keep this beam unchanged.\n",
        "                        if seq[-1].item() == eos_token:\n",
        "                            temp_beams.append((seq, score, h))\n",
        "                            continue\n",
        "\n",
        "                        # Generate the next token probabilities.\n",
        "                        last_token = seq[-1].unsqueeze(0).unsqueeze(0)\n",
        "                        embedded = self.embedding(last_token)\n",
        "                        embedded = self.dropout(embedded)\n",
        "                        output, h_new = self.decoder(embedded, h)\n",
        "                        logits = self.fc(output.squeeze(1))\n",
        "                        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "                        # Select top beam_width tokens.\n",
        "                        topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
        "                        for k in range(beam_width):\n",
        "                            new_seq = torch.cat([seq, topk_indices[0, k].unsqueeze(0)], dim=0)\n",
        "                            new_score = score + topk_log_probs[0, k].item()\n",
        "                            temp_beams.append((new_seq, new_score, h_new))\n",
        "\n",
        "                    temp_beams = sorted(temp_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "                    new_beams.append(temp_beams)\n",
        "                    if any(b[0][-1].item() != eos_token for b in temp_beams):\n",
        "                        all_finished = False\n",
        "\n",
        "                beams = new_beams\n",
        "                if all_finished:\n",
        "                    break\n",
        "\n",
        "            # Choose the best beam from each sample.\n",
        "            for i in range(batch_size):\n",
        "                best_seq, best_score, _ = sorted(beams[i], key=lambda x: x[1], reverse=True)[0]\n",
        "                final_outputs[i] = best_seq\n",
        "\n",
        "            # Return a list of 1D tensors (each sequence can have different lengths).\n",
        "            return final_outputs\n",
        "\n",
        "# --------------------- FMAD Training Functions using autograd ---------------------\n",
        "def train_model_fmad(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    epoch_start = time.time()\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader, 1):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        target_ids = batch['target_ids'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Use mixed precision for the forward pass and loss computation.\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(input_ids, target_ids[:, :-1])\n",
        "            loss = criterion(output.transpose(1, 2), target_ids[:, 1:])\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute gradients using autograd.grad to avoid reference cycles.\n",
        "        # (create_graph=True is maintained to mimic FMAD style.)\n",
        "        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
        "        for param, grad in zip(model.parameters(), grads):\n",
        "            param.grad = grad\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 500 == 0 or batch_idx == num_batches:\n",
        "            print(f\"  Batch {batch_idx}/{num_batches}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    epoch_end = time.time()\n",
        "    epoch_duration = epoch_end - epoch_start\n",
        "    print(f\"FMAD epoch completed in {epoch_duration:.2f} seconds.\")\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(\"FMAD Training Loss for epoch:\", avg_loss)\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate_model(model, dataloader, tokenizer, device, sos_token, eos_token):\n",
        "    model.eval()\n",
        "    rouge = ROUGEScore()\n",
        "    predictions = []\n",
        "    references = []\n",
        "    eval_start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            target_ids = batch['target_ids'].to(device)\n",
        "            outputs = model.generate(input_ids, sos_token, eos_token, beam_width=3)\n",
        "            for pred_ids, tgt_ids in zip(outputs, target_ids):\n",
        "                pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
        "                tgt_text = tokenizer.decode(tgt_ids, skip_special_tokens=True)\n",
        "                predictions.append(pred_text)\n",
        "                references.append(tgt_text)\n",
        "    scores = rouge(predictions, references)\n",
        "    eval_end = time.time()\n",
        "    eval_duration = eval_end - eval_start\n",
        "    print(f\"Evaluation completed in {eval_duration:.2f} seconds.\")\n",
        "    print(\"Evaluation ROUGE scores:\", scores)\n",
        "    return scores\n",
        "\n",
        "# --------------------- Setup and Training ---------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "pad_idx = tokenizer.pad_token_id\n",
        "sos_token = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id\n",
        "eos_token = tokenizer.eos_token_id\n",
        "\n",
        "# Hyperparameters\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers = 1\n",
        "dropout_rate = 0.3\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 0.003\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "# Load 50% of each split of the CNN/DailyMail dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(len(dataset[\"train\"]) // 2))\n",
        "val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(len(dataset[\"validation\"]) // 2))\n",
        "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(len(dataset[\"test\"]) // 2))\n",
        "\n",
        "train_data = SummarizationDataset(train_dataset, tokenizer)\n",
        "val_data = SummarizationDataset(val_dataset, tokenizer)\n",
        "test_data = SummarizationDataset(test_dataset, tokenizer)\n",
        "\n",
        "# Optimize DataLoader: use more workers and pin memory.\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = Seq2SeqGRUModel(vocab_size, embed_dim, hidden_dim, num_layers, pad_idx, dropout=dropout_rate).to(device)\n",
        "\n",
        "print(\"Starting FMAD-based training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n=== FMAD Epoch {epoch + 1}/{num_epochs} ===\")\n",
        "    epoch_start_time = time.time()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    train_loss = train_model_fmad(model, train_loader, optimizer, criterion, device)\n",
        "    epoch_end_time = time.time()\n",
        "    total_epoch_time = epoch_end_time - epoch_start_time\n",
        "    minutes = total_epoch_time // 60\n",
        "    seconds = total_epoch_time % 60\n",
        "    print(f\"Epoch {epoch + 1} finished in {int(minutes)} minutes {seconds:.2f} seconds.\")\n",
        "    print(f\"FMAD Training Loss: {train_loss:.4f}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMMHWPTJZGkb",
        "outputId": "ec98dc68-7615-4e3f-c5fd-e1c37bbd23c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on validation data...\n",
            "Evaluation completed in 490.17 seconds.\n",
            "Evaluation ROUGE scores: {'rouge1_fmeasure': tensor(0.1198), 'rouge1_precision': tensor(0.2075), 'rouge1_recall': tensor(0.0891), 'rouge2_fmeasure': tensor(0.0127), 'rouge2_precision': tensor(0.0228), 'rouge2_recall': tensor(0.0093), 'rougeL_fmeasure': tensor(0.0966), 'rougeL_precision': tensor(0.1668), 'rougeL_recall': tensor(0.0719), 'rougeLsum_fmeasure': tensor(0.1088), 'rougeLsum_precision': tensor(0.1885), 'rougeLsum_recall': tensor(0.0808)}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1_fmeasure': tensor(0.1198),\n",
              " 'rouge1_precision': tensor(0.2075),\n",
              " 'rouge1_recall': tensor(0.0891),\n",
              " 'rouge2_fmeasure': tensor(0.0127),\n",
              " 'rouge2_precision': tensor(0.0228),\n",
              " 'rouge2_recall': tensor(0.0093),\n",
              " 'rougeL_fmeasure': tensor(0.0966),\n",
              " 'rougeL_precision': tensor(0.1668),\n",
              " 'rougeL_recall': tensor(0.0719),\n",
              " 'rougeLsum_fmeasure': tensor(0.1088),\n",
              " 'rougeLsum_precision': tensor(0.1885),\n",
              " 'rougeLsum_recall': tensor(0.0808)}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Evaluating on validation data...\")\n",
        "evaluate_model(model, test_loader, tokenizer, device, sos_token, eos_token)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
